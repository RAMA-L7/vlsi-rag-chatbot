# ğŸ§  RAG Architecture â€” How This Chatbot Works

This chatbot is built using the **Retrieval-Augmented Generation (RAG)** framework. Instead of relying solely on the language model's memory, it **retrieves relevant chunks from your own documents** and feeds them into the local LLM to answer your question.

---

## ğŸ—ï¸ Overall Architecture Flow

[User Query]
â†“
[Embed Query]
â†“
[ChromaDB] â†â†’ [Pre-embedded Chunks from Docs]
â†“
[Top-K Matches]
â†“
[LLM Prompt: Query + Retrieved Chunks]
â†“
[Local Answer Generated]

---

## ğŸ”§ Main Components

### 1. **Document Ingestion Pipeline (`ingest.py`)**

When you run `ingest.py`, the following steps happen:

| Step | Description |
|------|-------------|
| ğŸ” Read Files | Loads `.pdf`, `.docx`, `.pptx`, `.txt` from `data/` folder |
| ğŸ“ƒ Text Splitting | Breaks large text into small chunks (with context overlap) |
| ğŸ”¤ Embedding | Converts each chunk into a vector using `sentence-transformers` |
| ğŸ’¾ ChromaDB | Stores all vectors and metadata into a local persistent DB |

---

### 2. **Query Handling Pipeline (`chatbot.py`)**

When the user asks a question:

| Step | Description |
|------|-------------|
| ğŸ§  Embed Query | Converts user question into a vector |
| ğŸ“¡ Vector Search | Finds the most relevant chunks from ChromaDB |
| ğŸ§© Construct Prompt | Concatenates: system prompt + top-k chunks + user question |
| ğŸ¦™ LLM Response | Local LLM (Mistral-7B) generates the final answer |

---

## ğŸ¤– Why Use RAG?

- LLMs (even GPT-4) **donâ€™t know your documents**
- RAG makes them â€œappear smartâ€ by **feeding them real context**
- You get **custom answers without fine-tuning or retraining**

---

## ğŸ§  Local LLM: `llama-cpp-python`

- Model format: `.gguf` (Quantized Mistral-7B Instruct)
- Run via: `llama_cpp.Llama` class
- Benefits:
  - 100% offline
  - Runs on CPU (AVX2+)
  - Fast inference with quantized model (Q4_0)

---

## ğŸ“¦ Vector Store: ChromaDB

- Light, fast, and local
- Uses DuckDB under the hood
- Persists embeddings in `./chromadb/` folder

---

## ğŸ”¡ Embeddings: `sentence-transformers`

- Model: `all-MiniLM-L6-v2`
- Fast and small (384 or 768 dims)
- Converts all text into numerical vectors for comparison

---

## ğŸ§ª Prompt Format Example

```text
System: You are a VLSI design assistant. Use only the provided context to answer.
---
Context:
1. "A decap cell is used to provide local capacitance and reduce IR drop..."
2. "Tie-high and tie-low cells help avoid floating nodes in digital logic..."
---
Question: What is the function of tie-low cells?
